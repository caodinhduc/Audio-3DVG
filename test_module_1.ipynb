{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d41d2a92-7db3-4e8b-b60c-06d47b3f0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedAttention(nn.Module):\n",
    "    def __init__(self, dim_a, dim_b, latent_dim=256, heads=8):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.heads = heads\n",
    "        self.scale = (latent_dim // heads) ** -0.5\n",
    "\n",
    "        self.to_q_aa = nn.Linear(dim_a, latent_dim)\n",
    "        self.to_k_aa = nn.Linear(dim_a, latent_dim)\n",
    "        \n",
    "        self.to_v_a = nn.Linear(dim_a, latent_dim)\n",
    "\n",
    "        self.to_k_ab = nn.Linear(dim_a, latent_dim)\n",
    "        self.to_q_bb = nn.Linear(dim_b, latent_dim)\n",
    "\n",
    "        self.out = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        Bsz, N, _ = B.shape\n",
    "        H = self.heads\n",
    "        d_head = self.latent_dim // H\n",
    "\n",
    "        # Linear projections\n",
    "        V = self.to_v_a(A)\n",
    "        Q_self = self.to_q_aa(A)  # [B, N, D]\n",
    "        K_self = self.to_k_aa(A)\n",
    "        \n",
    "\n",
    "        Q_cross = self.to_k_ab(A)  # Cross-attention\n",
    "        K_cross = self.to_q_bb(B)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        def split_heads(x):  # [B, seq, D] -> [B, H, seq, d_head]\n",
    "            return x.view(Bsz, -1, H, d_head).transpose(1, 2)\n",
    "\n",
    "        V = split_heads(V)\n",
    "        Q_self = split_heads(Q_self)\n",
    "        K_self = split_heads(K_self)\n",
    "        Q_cross = split_heads(Q_cross)\n",
    "        K_cross = split_heads(K_cross)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_self = torch.matmul(Q_self, K_self.transpose(-2, -1)) * self.scale  # [B, H, N, N]\n",
    "        attn_cross = torch.matmul(Q_cross, K_cross.transpose(-2, -1)) * self.scale  # [B, H, N, N]\n",
    "\n",
    "        # Pad attn_cross to shape [B, H, N, N] if needed\n",
    "        # if M != N:\n",
    "        #     diff = N - M\n",
    "        #     pad = (0, diff)  # pad last dim\n",
    "        #     attn_cross = F.pad(attn_cross, pad, \"constant\", 0)\n",
    "\n",
    "        # Final attention: element-wise sum\n",
    "        attn = attn_self + attn_cross\n",
    "\n",
    "        # Softmax\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # print(attn.shape)\n",
    "\n",
    "        # Again pad V_cross if needed\n",
    "        # if M != N:\n",
    "        #     V_cross = F.pad(V_cross, (0, 0, 0, diff), \"constant\", 0)\n",
    "\n",
    "        # Combine values\n",
    "        out_attn = torch.matmul(attn, V)  # [B, H, N, d_head]\n",
    "\n",
    "        # Merge heads\n",
    "        out = out_attn.transpose(1, 2).contiguous().view(Bsz, N, self.latent_dim)\n",
    "\n",
    "        return self.out(out)  # Final projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f832f1a5-38df-4ef1-b0a5-da4ebb106a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones(8, 16, 2048)\n",
    "B = torch.ones(8, 16, 1074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cdcde80-d75f-4fd0-9267-d643bf326c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention = CombinedAttention(dim_a=2048, dim_b=1074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fcbc692-d8d7-4110-ac53-90ea67aa7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Attention(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a68d8c26-200b-482d-8679-964c2033aab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adf87733-1dca-4189-a293-30797a0ab8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         ...,\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238]],\n",
       "\n",
       "        [[ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         ...,\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238]],\n",
       "\n",
       "        [[ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         ...,\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         ...,\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238]],\n",
       "\n",
       "        [[ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         ...,\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238]],\n",
       "\n",
       "        [[ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         ...,\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238],\n",
       "         [ 0.1026, -0.2201,  0.5353,  ..., -0.0788,  0.2026, -0.0238]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223ba97-031b-430a-96dc-0846a5ebfefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
